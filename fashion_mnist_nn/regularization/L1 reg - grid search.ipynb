{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64414a72-7408-422b-bb6f-5129024ee08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sgd_L1 import NeuralNetwork\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "with open(\"../data/train_data.pkl\", \"rb\") as train_file:\n",
    "    train_data = pickle.load(train_file)\n",
    "\n",
    "rng.shuffle(train_data)\n",
    "validation_data = train_data[:5000]\n",
    "train_data = train_data[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b9cb7-834f-4655-a89c-fff8caf00b31",
   "metadata": {},
   "source": [
    "### Upper bound\n",
    "\n",
    "At $\\lambda=10$ it gets so \"regulirized\" that stucks at a \"random selector\" minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5452e8dc-7269-4207-a256-3e3ce8f74c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train stats\n",
      "==> Accuracy: 8.5%, Avg loss: 5.972364\n",
      "\n",
      "Epoch 0\n",
      "--------------------\n",
      "loss: 6.705551 [mini-batch 0 / 859]\n",
      "loss: 1.418251 [mini-batch 100 / 859]\n",
      "loss: 2.296339 [mini-batch 200 / 859]\n",
      "loss: 2.308395 [mini-batch 300 / 859]\n",
      "loss: 2.305484 [mini-batch 400 / 859]\n",
      "loss: 2.304683 [mini-batch 500 / 859]\n",
      "loss: 2.306601 [mini-batch 600 / 859]\n",
      "loss: 2.301514 [mini-batch 700 / 859]\n",
      "loss: 2.305207 [mini-batch 800 / 859]\n",
      "==> Accuracy: 9.8%, Avg loss: 2.303608\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 2.302784 [mini-batch 0 / 859]\n",
      "loss: 2.302857 [mini-batch 100 / 859]\n",
      "loss: 2.294295 [mini-batch 200 / 859]\n",
      "loss: 2.303978 [mini-batch 300 / 859]\n",
      "loss: 2.309819 [mini-batch 400 / 859]\n",
      "loss: 2.306653 [mini-batch 500 / 859]\n",
      "loss: 2.302232 [mini-batch 600 / 859]\n",
      "loss: 2.306193 [mini-batch 700 / 859]\n",
      "loss: 2.302693 [mini-batch 800 / 859]\n",
      "==> Accuracy: 9.8%, Avg loss: 2.303483\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0846, 0.0976, 0.0976],\n",
       " [5.972364226390625, 2.303608148055029, 2.303483041974756])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get upper bound\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    [28*28, 1024, 512, 128, 10], # layers size\n",
    "    1e-3,                        # learning rate\n",
    "    10e-0,                       # L1 lambda\n",
    "    64,                          # mini batch size\n",
    "    2                            # training epochs\n",
    ")\n",
    "\n",
    "nn.train(train_data, validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ab977-1739-4965-aaf5-a9f8dde95bf7",
   "metadata": {},
   "source": [
    "### Grid search (single sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75659b3-3a78-4c55-9039-3285519b9255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train stats\n",
      "==> Accuracy: 11.0%, Avg loss: 4.962476\n",
      "\n",
      "Epoch 0\n",
      "--------------------\n",
      "loss: 4.979127 [mini-batch 0 / 859]\n",
      "loss: 0.781828 [mini-batch 100 / 859]\n",
      "loss: 0.494548 [mini-batch 200 / 859]\n",
      "loss: 0.562200 [mini-batch 300 / 859]\n",
      "loss: 0.579491 [mini-batch 400 / 859]\n",
      "loss: 0.628759 [mini-batch 500 / 859]\n",
      "loss: 0.427479 [mini-batch 600 / 859]\n",
      "loss: 0.528160 [mini-batch 700 / 859]\n",
      "loss: 0.365394 [mini-batch 800 / 859]\n",
      "==> Accuracy: 83.2%, Avg loss: 0.481751\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 0.299071 [mini-batch 0 / 859]\n",
      "loss: 0.401725 [mini-batch 100 / 859]\n",
      "loss: 0.486557 [mini-batch 200 / 859]\n",
      "loss: 0.442566 [mini-batch 300 / 859]\n",
      "loss: 0.489084 [mini-batch 400 / 859]\n",
      "loss: 0.417864 [mini-batch 500 / 859]\n",
      "loss: 0.435142 [mini-batch 600 / 859]\n",
      "loss: 0.249554 [mini-batch 700 / 859]\n",
      "loss: 0.481318 [mini-batch 800 / 859]\n",
      "==> Accuracy: 84.8%, Avg loss: 0.443310\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 0.234713 [mini-batch 0 / 859]\n",
      "loss: 0.463540 [mini-batch 100 / 859]\n",
      "loss: 0.377183 [mini-batch 200 / 859]\n",
      "loss: 0.418696 [mini-batch 300 / 859]\n",
      "loss: 0.368029 [mini-batch 400 / 859]\n",
      "loss: 0.677798 [mini-batch 500 / 859]\n",
      "loss: 0.310923 [mini-batch 600 / 859]\n",
      "loss: 0.447974 [mini-batch 700 / 859]\n",
      "loss: 0.364314 [mini-batch 800 / 859]\n",
      "==> Accuracy: 84.8%, Avg loss: 0.424667\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 0.512393 [mini-batch 0 / 859]\n",
      "loss: 0.314776 [mini-batch 100 / 859]\n",
      "loss: 0.507805 [mini-batch 200 / 859]\n",
      "loss: 0.569672 [mini-batch 300 / 859]\n",
      "loss: 0.351573 [mini-batch 400 / 859]\n",
      "loss: 0.527806 [mini-batch 500 / 859]\n",
      "loss: 0.429264 [mini-batch 600 / 859]\n",
      "loss: 0.372202 [mini-batch 700 / 859]\n",
      "loss: 0.331388 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.8%, Avg loss: 0.406472\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 0.297815 [mini-batch 0 / 859]\n",
      "loss: 0.310852 [mini-batch 100 / 859]\n",
      "loss: 0.385161 [mini-batch 200 / 859]\n",
      "loss: 0.464868 [mini-batch 300 / 859]\n",
      "loss: 0.453897 [mini-batch 400 / 859]\n",
      "loss: 0.323929 [mini-batch 500 / 859]\n",
      "loss: 0.258266 [mini-batch 600 / 859]\n",
      "loss: 0.400498 [mini-batch 700 / 859]\n",
      "loss: 0.572062 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.4%, Avg loss: 0.414557\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 0.462759 [mini-batch 0 / 859]\n",
      "loss: 0.410329 [mini-batch 100 / 859]\n",
      "loss: 0.597842 [mini-batch 200 / 859]\n",
      "loss: 0.361173 [mini-batch 300 / 859]\n",
      "loss: 0.412572 [mini-batch 400 / 859]\n",
      "loss: 0.460361 [mini-batch 500 / 859]\n",
      "loss: 0.470815 [mini-batch 600 / 859]\n",
      "loss: 0.486320 [mini-batch 700 / 859]\n",
      "loss: 0.464858 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.9%, Avg loss: 0.405201\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 0.515788 [mini-batch 0 / 859]\n",
      "loss: 0.443440 [mini-batch 100 / 859]\n",
      "loss: 0.468431 [mini-batch 200 / 859]\n",
      "loss: 0.431287 [mini-batch 300 / 859]\n",
      "loss: 0.728644 [mini-batch 400 / 859]\n",
      "loss: 0.305118 [mini-batch 500 / 859]\n",
      "loss: 0.322707 [mini-batch 600 / 859]\n",
      "loss: 0.233213 [mini-batch 700 / 859]\n",
      "loss: 0.406694 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.6%, Avg loss: 0.404880\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 0.237924 [mini-batch 0 / 859]\n",
      "loss: 0.455657 [mini-batch 100 / 859]\n",
      "loss: 0.287283 [mini-batch 200 / 859]\n",
      "loss: 0.310421 [mini-batch 300 / 859]\n",
      "loss: 0.314869 [mini-batch 400 / 859]\n",
      "loss: 0.467886 [mini-batch 500 / 859]\n",
      "loss: 0.486360 [mini-batch 600 / 859]\n",
      "loss: 0.550518 [mini-batch 700 / 859]\n",
      "loss: 0.320933 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.7%, Avg loss: 0.410686\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 0.383141 [mini-batch 0 / 859]\n",
      "loss: 0.482738 [mini-batch 100 / 859]\n",
      "loss: 0.480655 [mini-batch 200 / 859]\n",
      "loss: 0.470334 [mini-batch 300 / 859]\n",
      "loss: 0.310524 [mini-batch 400 / 859]\n",
      "loss: 0.521568 [mini-batch 500 / 859]\n",
      "loss: 0.458204 [mini-batch 600 / 859]\n",
      "loss: 0.321611 [mini-batch 700 / 859]\n",
      "loss: 0.336548 [mini-batch 800 / 859]\n",
      "==> Accuracy: 83.6%, Avg loss: 0.455787\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 0.252413 [mini-batch 0 / 859]\n",
      "loss: 0.536420 [mini-batch 100 / 859]\n",
      "loss: 0.439432 [mini-batch 200 / 859]\n",
      "loss: 0.394193 [mini-batch 300 / 859]\n",
      "loss: 0.306190 [mini-batch 400 / 859]\n",
      "loss: 0.515174 [mini-batch 500 / 859]\n",
      "loss: 0.310057 [mini-batch 600 / 859]\n",
      "loss: 0.478369 [mini-batch 700 / 859]\n",
      "loss: 0.286728 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.4%, Avg loss: 0.389992\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 0.399828 [mini-batch 0 / 859]\n",
      "loss: 0.320544 [mini-batch 100 / 859]\n",
      "loss: 0.438497 [mini-batch 200 / 859]\n",
      "loss: 0.304815 [mini-batch 300 / 859]\n",
      "loss: 0.245749 [mini-batch 400 / 859]\n",
      "loss: 0.390393 [mini-batch 500 / 859]\n",
      "loss: 0.435863 [mini-batch 600 / 859]\n",
      "loss: 0.394079 [mini-batch 700 / 859]\n",
      "loss: 0.323921 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.2%, Avg loss: 0.402340\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "loss: 0.269482 [mini-batch 0 / 859]\n",
      "loss: 0.350936 [mini-batch 100 / 859]\n",
      "loss: 0.354711 [mini-batch 200 / 859]\n",
      "loss: 0.378953 [mini-batch 300 / 859]\n",
      "loss: 0.396931 [mini-batch 400 / 859]\n",
      "loss: 0.394318 [mini-batch 500 / 859]\n",
      "loss: 0.395569 [mini-batch 600 / 859]\n",
      "loss: 0.470971 [mini-batch 700 / 859]\n",
      "loss: 0.241009 [mini-batch 800 / 859]\n",
      "==> Accuracy: 84.5%, Avg loss: 0.424625\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "loss: 0.334956 [mini-batch 0 / 859]\n",
      "loss: 0.322305 [mini-batch 100 / 859]\n",
      "loss: 0.480529 [mini-batch 200 / 859]\n",
      "loss: 0.569490 [mini-batch 300 / 859]\n",
      "loss: 0.422294 [mini-batch 400 / 859]\n",
      "loss: 0.385551 [mini-batch 500 / 859]\n",
      "loss: 0.316859 [mini-batch 600 / 859]\n",
      "loss: 0.419715 [mini-batch 700 / 859]\n",
      "loss: 0.155225 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.9%, Avg loss: 0.397339\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "loss: 0.354929 [mini-batch 0 / 859]\n",
      "loss: 0.385078 [mini-batch 100 / 859]\n",
      "loss: 0.397468 [mini-batch 200 / 859]\n",
      "loss: 0.297496 [mini-batch 300 / 859]\n",
      "loss: 0.328132 [mini-batch 400 / 859]\n",
      "loss: 0.333588 [mini-batch 500 / 859]\n",
      "loss: 0.331639 [mini-batch 600 / 859]\n",
      "loss: 0.339381 [mini-batch 700 / 859]\n",
      "loss: 0.497873 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.9%, Avg loss: 0.406998\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "loss: 0.200331 [mini-batch 0 / 859]\n",
      "loss: 0.521560 [mini-batch 100 / 859]\n",
      "loss: 0.277425 [mini-batch 200 / 859]\n",
      "loss: 0.447402 [mini-batch 300 / 859]\n",
      "loss: 0.441985 [mini-batch 400 / 859]\n",
      "loss: 0.155928 [mini-batch 500 / 859]\n",
      "loss: 0.308102 [mini-batch 600 / 859]\n",
      "loss: 0.484237 [mini-batch 700 / 859]\n",
      "loss: 0.300571 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.2%, Avg loss: 0.397404\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "loss: 0.234386 [mini-batch 0 / 859]\n",
      "loss: 0.292538 [mini-batch 100 / 859]\n",
      "loss: 0.488379 [mini-batch 200 / 859]\n",
      "loss: 0.443904 [mini-batch 300 / 859]\n",
      "loss: 0.538505 [mini-batch 400 / 859]\n",
      "loss: 0.367025 [mini-batch 500 / 859]\n",
      "loss: 0.246796 [mini-batch 600 / 859]\n",
      "loss: 0.282485 [mini-batch 700 / 859]\n",
      "loss: 0.464675 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.0%, Avg loss: 0.405155\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "loss: 0.667369 [mini-batch 0 / 859]\n",
      "loss: 0.397826 [mini-batch 100 / 859]\n",
      "loss: 0.377468 [mini-batch 200 / 859]\n",
      "loss: 0.337001 [mini-batch 300 / 859]\n",
      "loss: 0.439275 [mini-batch 400 / 859]\n",
      "loss: 0.467154 [mini-batch 500 / 859]\n",
      "loss: 0.506698 [mini-batch 600 / 859]\n",
      "loss: 0.420940 [mini-batch 700 / 859]\n",
      "loss: 0.480165 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.1%, Avg loss: 0.402670\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "loss: 0.213123 [mini-batch 0 / 859]\n",
      "loss: 0.360416 [mini-batch 100 / 859]\n",
      "loss: 0.336521 [mini-batch 200 / 859]\n",
      "loss: 0.309546 [mini-batch 300 / 859]\n",
      "loss: 0.557791 [mini-batch 400 / 859]\n",
      "loss: 0.411979 [mini-batch 500 / 859]\n",
      "loss: 0.323027 [mini-batch 600 / 859]\n",
      "loss: 0.457482 [mini-batch 700 / 859]\n",
      "loss: 0.486418 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.7%, Avg loss: 0.394633\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "loss: 0.324807 [mini-batch 0 / 859]\n",
      "loss: 0.360331 [mini-batch 100 / 859]\n",
      "loss: 0.321270 [mini-batch 200 / 859]\n",
      "loss: 0.329985 [mini-batch 300 / 859]\n",
      "loss: 0.438445 [mini-batch 400 / 859]\n",
      "loss: 0.572136 [mini-batch 500 / 859]\n",
      "loss: 0.297290 [mini-batch 600 / 859]\n",
      "loss: 0.357728 [mini-batch 700 / 859]\n",
      "loss: 0.623439 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.4%, Avg loss: 0.400165\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "loss: 0.418404 [mini-batch 0 / 859]\n",
      "loss: 0.268548 [mini-batch 100 / 859]\n",
      "loss: 0.261611 [mini-batch 200 / 859]\n",
      "loss: 0.317695 [mini-batch 300 / 859]\n",
      "loss: 0.273857 [mini-batch 400 / 859]\n",
      "loss: 0.468454 [mini-batch 500 / 859]\n",
      "loss: 0.284664 [mini-batch 600 / 859]\n",
      "loss: 0.353326 [mini-batch 700 / 859]\n",
      "loss: 0.348935 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.6%, Avg loss: 0.416648\n",
      "\n",
      "Pre-train stats\n",
      "==> Accuracy: 11.0%, Avg loss: 6.417140\n",
      "\n",
      "Epoch 0\n",
      "--------------------\n",
      "loss: 6.477262 [mini-batch 0 / 859]\n",
      "loss: 0.874068 [mini-batch 100 / 859]\n",
      "loss: 0.643027 [mini-batch 200 / 859]\n",
      "loss: 0.553179 [mini-batch 300 / 859]\n",
      "loss: 0.568752 [mini-batch 400 / 859]\n",
      "loss: 0.727071 [mini-batch 500 / 859]\n",
      "loss: 0.453917 [mini-batch 600 / 859]\n",
      "loss: 0.402850 [mini-batch 700 / 859]\n",
      "loss: 0.433937 [mini-batch 800 / 859]\n",
      "==> Accuracy: 82.1%, Avg loss: 0.493369\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 0.504397 [mini-batch 0 / 859]\n",
      "loss: 0.564003 [mini-batch 100 / 859]\n",
      "loss: 0.311502 [mini-batch 200 / 859]\n",
      "loss: 0.493529 [mini-batch 300 / 859]\n",
      "loss: 0.393135 [mini-batch 400 / 859]\n",
      "loss: 0.561729 [mini-batch 500 / 859]\n",
      "loss: 0.255968 [mini-batch 600 / 859]\n",
      "loss: 0.474033 [mini-batch 700 / 859]\n",
      "loss: 0.555101 [mini-batch 800 / 859]\n",
      "==> Accuracy: 84.2%, Avg loss: 0.441475\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 0.527884 [mini-batch 0 / 859]\n",
      "loss: 0.365890 [mini-batch 100 / 859]\n",
      "loss: 0.417742 [mini-batch 200 / 859]\n",
      "loss: 0.512986 [mini-batch 300 / 859]\n",
      "loss: 0.446786 [mini-batch 400 / 859]\n",
      "loss: 0.482150 [mini-batch 500 / 859]\n",
      "loss: 0.537180 [mini-batch 600 / 859]\n",
      "loss: 0.372152 [mini-batch 700 / 859]\n",
      "loss: 0.250380 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.0%, Avg loss: 0.432857\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 0.490475 [mini-batch 0 / 859]\n",
      "loss: 0.351114 [mini-batch 100 / 859]\n",
      "loss: 0.433057 [mini-batch 200 / 859]\n",
      "loss: 0.404054 [mini-batch 300 / 859]\n",
      "loss: 0.232247 [mini-batch 400 / 859]\n",
      "loss: 0.455252 [mini-batch 500 / 859]\n",
      "loss: 0.426566 [mini-batch 600 / 859]\n",
      "loss: 0.388305 [mini-batch 700 / 859]\n",
      "loss: 0.431044 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.8%, Avg loss: 0.405447\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 0.432163 [mini-batch 0 / 859]\n",
      "loss: 0.371165 [mini-batch 100 / 859]\n",
      "loss: 0.419433 [mini-batch 200 / 859]\n",
      "loss: 0.363247 [mini-batch 300 / 859]\n",
      "loss: 0.474812 [mini-batch 400 / 859]\n",
      "loss: 0.496430 [mini-batch 500 / 859]\n",
      "loss: 0.367160 [mini-batch 600 / 859]\n",
      "loss: 0.573041 [mini-batch 700 / 859]\n",
      "loss: 0.470400 [mini-batch 800 / 859]\n",
      "==> Accuracy: 83.6%, Avg loss: 0.464293\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 0.344414 [mini-batch 0 / 859]\n",
      "loss: 0.488337 [mini-batch 100 / 859]\n",
      "loss: 0.295581 [mini-batch 200 / 859]\n",
      "loss: 0.365387 [mini-batch 300 / 859]\n",
      "loss: 0.310046 [mini-batch 400 / 859]\n",
      "loss: 0.321956 [mini-batch 500 / 859]\n",
      "loss: 0.414899 [mini-batch 600 / 859]\n",
      "loss: 0.362663 [mini-batch 700 / 859]\n",
      "loss: 0.585770 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.3%, Avg loss: 0.412675\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 0.302050 [mini-batch 0 / 859]\n",
      "loss: 0.458620 [mini-batch 100 / 859]\n",
      "loss: 0.317859 [mini-batch 200 / 859]\n",
      "loss: 0.524700 [mini-batch 300 / 859]\n",
      "loss: 0.370091 [mini-batch 400 / 859]\n",
      "loss: 0.465951 [mini-batch 500 / 859]\n",
      "loss: 0.610264 [mini-batch 600 / 859]\n",
      "loss: 0.313229 [mini-batch 700 / 859]\n",
      "loss: 0.304517 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.0%, Avg loss: 0.402327\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 0.186584 [mini-batch 0 / 859]\n",
      "loss: 0.473493 [mini-batch 100 / 859]\n",
      "loss: 0.368439 [mini-batch 200 / 859]\n",
      "loss: 0.571726 [mini-batch 300 / 859]\n",
      "loss: 0.345761 [mini-batch 400 / 859]\n",
      "loss: 0.527920 [mini-batch 500 / 859]\n",
      "loss: 0.575890 [mini-batch 600 / 859]\n",
      "loss: 0.375177 [mini-batch 700 / 859]\n",
      "loss: 0.536220 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.9%, Avg loss: 0.392570\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 0.455805 [mini-batch 0 / 859]\n",
      "loss: 0.275977 [mini-batch 100 / 859]\n",
      "loss: 0.411358 [mini-batch 200 / 859]\n",
      "loss: 0.419600 [mini-batch 300 / 859]\n",
      "loss: 0.330664 [mini-batch 400 / 859]\n",
      "loss: 0.440507 [mini-batch 500 / 859]\n",
      "loss: 0.343137 [mini-batch 600 / 859]\n",
      "loss: 0.227676 [mini-batch 700 / 859]\n",
      "loss: 0.397132 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.0%, Avg loss: 0.399812\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 0.520277 [mini-batch 0 / 859]\n",
      "loss: 0.409865 [mini-batch 100 / 859]\n",
      "loss: 0.246372 [mini-batch 200 / 859]\n",
      "loss: 0.513569 [mini-batch 300 / 859]\n",
      "loss: 0.389457 [mini-batch 400 / 859]\n",
      "loss: 0.378043 [mini-batch 500 / 859]\n",
      "loss: 0.392692 [mini-batch 600 / 859]\n",
      "loss: 0.315924 [mini-batch 700 / 859]\n",
      "loss: 0.386031 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.4%, Avg loss: 0.395922\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 0.257113 [mini-batch 0 / 859]\n",
      "loss: 0.689001 [mini-batch 100 / 859]\n",
      "loss: 0.542782 [mini-batch 200 / 859]\n",
      "loss: 0.338455 [mini-batch 300 / 859]\n",
      "loss: 0.446808 [mini-batch 400 / 859]\n",
      "loss: 0.292832 [mini-batch 500 / 859]\n",
      "loss: 0.362146 [mini-batch 600 / 859]\n",
      "loss: 0.435230 [mini-batch 700 / 859]\n",
      "loss: 0.369131 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.4%, Avg loss: 0.403630\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "loss: 0.515412 [mini-batch 0 / 859]\n",
      "loss: 0.247728 [mini-batch 100 / 859]\n",
      "loss: 0.283822 [mini-batch 200 / 859]\n",
      "loss: 0.481705 [mini-batch 300 / 859]\n",
      "loss: 0.273306 [mini-batch 400 / 859]\n",
      "loss: 0.226722 [mini-batch 500 / 859]\n",
      "loss: 0.275472 [mini-batch 600 / 859]\n",
      "loss: 0.435806 [mini-batch 700 / 859]\n",
      "loss: 0.495575 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.5%, Avg loss: 0.392889\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "loss: 0.443335 [mini-batch 0 / 859]\n",
      "loss: 0.530340 [mini-batch 100 / 859]\n",
      "loss: 0.469348 [mini-batch 200 / 859]\n",
      "loss: 0.577434 [mini-batch 300 / 859]\n",
      "loss: 0.281438 [mini-batch 400 / 859]\n",
      "loss: 0.514415 [mini-batch 500 / 859]\n",
      "loss: 0.289971 [mini-batch 600 / 859]\n",
      "loss: 0.402692 [mini-batch 700 / 859]\n",
      "loss: 0.392164 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.1%, Avg loss: 0.398446\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "loss: 0.231856 [mini-batch 0 / 859]\n",
      "loss: 0.333062 [mini-batch 100 / 859]\n",
      "loss: 0.484651 [mini-batch 200 / 859]\n",
      "loss: 0.381535 [mini-batch 300 / 859]\n",
      "loss: 0.257448 [mini-batch 400 / 859]\n",
      "loss: 0.377523 [mini-batch 500 / 859]\n",
      "loss: 0.234993 [mini-batch 600 / 859]\n",
      "loss: 0.287888 [mini-batch 700 / 859]\n",
      "loss: 0.445940 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.6%, Avg loss: 0.401907\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "loss: 0.403103 [mini-batch 0 / 859]\n",
      "loss: 0.284792 [mini-batch 100 / 859]\n",
      "loss: 0.596735 [mini-batch 200 / 859]\n",
      "loss: 0.413241 [mini-batch 300 / 859]\n",
      "loss: 0.435733 [mini-batch 400 / 859]\n",
      "loss: 0.586139 [mini-batch 500 / 859]\n",
      "loss: 0.439270 [mini-batch 600 / 859]\n",
      "loss: 0.416270 [mini-batch 700 / 859]\n",
      "loss: 0.327669 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.8%, Avg loss: 0.427490\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "loss: 0.396634 [mini-batch 0 / 859]\n",
      "loss: 0.419883 [mini-batch 100 / 859]\n",
      "loss: 0.723427 [mini-batch 200 / 859]\n",
      "loss: 0.313303 [mini-batch 300 / 859]\n",
      "loss: 0.488584 [mini-batch 400 / 859]\n",
      "loss: 0.281638 [mini-batch 500 / 859]\n",
      "loss: 0.478248 [mini-batch 600 / 859]\n",
      "loss: 0.342152 [mini-batch 700 / 859]\n",
      "loss: 0.237288 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.6%, Avg loss: 0.394717\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "loss: 0.375150 [mini-batch 0 / 859]\n",
      "loss: 0.372070 [mini-batch 100 / 859]\n",
      "loss: 0.343126 [mini-batch 200 / 859]\n",
      "loss: 0.465688 [mini-batch 300 / 859]\n",
      "loss: 0.354664 [mini-batch 400 / 859]\n",
      "loss: 0.452232 [mini-batch 500 / 859]\n",
      "loss: 0.422749 [mini-batch 600 / 859]\n",
      "loss: 0.251705 [mini-batch 700 / 859]\n",
      "loss: 0.242625 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.8%, Avg loss: 0.416619\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "loss: 0.688314 [mini-batch 0 / 859]\n",
      "loss: 0.339798 [mini-batch 100 / 859]\n",
      "loss: 0.406987 [mini-batch 200 / 859]\n",
      "loss: 0.262425 [mini-batch 300 / 859]\n",
      "loss: 0.406300 [mini-batch 400 / 859]\n",
      "loss: 0.363656 [mini-batch 500 / 859]\n",
      "loss: 0.421804 [mini-batch 600 / 859]\n",
      "loss: 0.405478 [mini-batch 700 / 859]\n",
      "loss: 0.468978 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.3%, Avg loss: 0.414675\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "loss: 0.387874 [mini-batch 0 / 859]\n",
      "loss: 0.512759 [mini-batch 100 / 859]\n",
      "loss: 0.473953 [mini-batch 200 / 859]\n",
      "loss: 0.307324 [mini-batch 300 / 859]\n",
      "loss: 0.380034 [mini-batch 400 / 859]\n",
      "loss: 0.339508 [mini-batch 500 / 859]\n",
      "loss: 0.488966 [mini-batch 600 / 859]\n",
      "loss: 0.334467 [mini-batch 700 / 859]\n",
      "loss: 0.461462 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.0%, Avg loss: 0.419513\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "loss: 0.248290 [mini-batch 0 / 859]\n",
      "loss: 0.490404 [mini-batch 100 / 859]\n",
      "loss: 0.286165 [mini-batch 200 / 859]\n",
      "loss: 0.215616 [mini-batch 300 / 859]\n",
      "loss: 0.176674 [mini-batch 400 / 859]\n",
      "loss: 0.404467 [mini-batch 500 / 859]\n",
      "loss: 0.503546 [mini-batch 600 / 859]\n",
      "loss: 0.221400 [mini-batch 700 / 859]\n",
      "loss: 0.407374 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.6%, Avg loss: 0.415973\n",
      "\n",
      "Pre-train stats\n",
      "==> Accuracy: 6.3%, Avg loss: 7.319981\n",
      "\n",
      "Epoch 0\n",
      "--------------------\n",
      "loss: 8.431293 [mini-batch 0 / 859]\n",
      "loss: 0.763372 [mini-batch 100 / 859]\n",
      "loss: 0.587091 [mini-batch 200 / 859]\n",
      "loss: 0.467492 [mini-batch 300 / 859]\n",
      "loss: 0.512684 [mini-batch 400 / 859]\n",
      "loss: 0.671460 [mini-batch 500 / 859]\n",
      "loss: 0.538514 [mini-batch 600 / 859]\n",
      "loss: 0.430717 [mini-batch 700 / 859]\n",
      "loss: 0.547959 [mini-batch 800 / 859]\n",
      "==> Accuracy: 81.2%, Avg loss: 0.528425\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 0.733907 [mini-batch 0 / 859]\n",
      "loss: 0.323999 [mini-batch 100 / 859]\n",
      "loss: 0.508631 [mini-batch 200 / 859]\n",
      "loss: 0.462167 [mini-batch 300 / 859]\n",
      "loss: 0.644124 [mini-batch 400 / 859]\n",
      "loss: 0.317723 [mini-batch 500 / 859]\n",
      "loss: 0.531697 [mini-batch 600 / 859]\n",
      "loss: 0.296696 [mini-batch 700 / 859]\n",
      "loss: 0.551589 [mini-batch 800 / 859]\n",
      "==> Accuracy: 84.6%, Avg loss: 0.447544\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 0.253564 [mini-batch 0 / 859]\n",
      "loss: 0.168386 [mini-batch 100 / 859]\n",
      "loss: 0.469877 [mini-batch 200 / 859]\n",
      "loss: 0.504679 [mini-batch 300 / 859]\n",
      "loss: 0.446629 [mini-batch 400 / 859]\n",
      "loss: 0.490406 [mini-batch 500 / 859]\n",
      "loss: 0.398166 [mini-batch 600 / 859]\n",
      "loss: 0.468842 [mini-batch 700 / 859]\n",
      "loss: 0.523688 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.8%, Avg loss: 0.425154\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 0.486098 [mini-batch 0 / 859]\n",
      "loss: 0.517959 [mini-batch 100 / 859]\n",
      "loss: 0.571771 [mini-batch 200 / 859]\n",
      "loss: 0.391965 [mini-batch 300 / 859]\n",
      "loss: 0.341678 [mini-batch 400 / 859]\n",
      "loss: 0.698348 [mini-batch 500 / 859]\n",
      "loss: 0.462261 [mini-batch 600 / 859]\n",
      "loss: 0.344729 [mini-batch 700 / 859]\n",
      "loss: 0.349889 [mini-batch 800 / 859]\n",
      "==> Accuracy: 84.5%, Avg loss: 0.446187\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 0.505585 [mini-batch 0 / 859]\n",
      "loss: 0.354658 [mini-batch 100 / 859]\n",
      "loss: 0.337834 [mini-batch 200 / 859]\n",
      "loss: 0.221138 [mini-batch 300 / 859]\n",
      "loss: 0.361516 [mini-batch 400 / 859]\n",
      "loss: 0.412063 [mini-batch 500 / 859]\n",
      "loss: 0.489459 [mini-batch 600 / 859]\n",
      "loss: 0.399971 [mini-batch 700 / 859]\n",
      "loss: 0.350041 [mini-batch 800 / 859]\n",
      "==> Accuracy: 86.0%, Avg loss: 0.410678\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 0.394693 [mini-batch 0 / 859]\n",
      "loss: 0.361398 [mini-batch 100 / 859]\n",
      "loss: 0.314121 [mini-batch 200 / 859]\n",
      "loss: 0.475326 [mini-batch 300 / 859]\n",
      "loss: 0.572589 [mini-batch 400 / 859]\n",
      "loss: 0.638125 [mini-batch 500 / 859]\n",
      "loss: 0.257215 [mini-batch 600 / 859]\n",
      "loss: 0.546129 [mini-batch 700 / 859]\n",
      "loss: 0.270646 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.3%, Avg loss: 0.413947\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 0.489738 [mini-batch 0 / 859]\n",
      "loss: 0.450054 [mini-batch 100 / 859]\n",
      "loss: 0.490741 [mini-batch 200 / 859]\n",
      "loss: 0.327858 [mini-batch 300 / 859]\n",
      "loss: 0.402084 [mini-batch 400 / 859]\n",
      "loss: 0.283208 [mini-batch 500 / 859]\n",
      "loss: 0.559872 [mini-batch 600 / 859]\n",
      "loss: 0.535073 [mini-batch 700 / 859]\n",
      "loss: 0.748490 [mini-batch 800 / 859]\n",
      "==> Accuracy: 84.6%, Avg loss: 0.431804\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 0.433666 [mini-batch 0 / 859]\n",
      "loss: 0.439767 [mini-batch 100 / 859]\n",
      "loss: 0.475623 [mini-batch 200 / 859]\n",
      "loss: 0.357194 [mini-batch 300 / 859]\n",
      "loss: 0.312314 [mini-batch 400 / 859]\n",
      "loss: 0.213283 [mini-batch 500 / 859]\n",
      "loss: 0.219286 [mini-batch 600 / 859]\n",
      "loss: 0.429947 [mini-batch 700 / 859]\n",
      "loss: 0.312647 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.7%, Avg loss: 0.412676\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 0.228317 [mini-batch 0 / 859]\n",
      "loss: 0.284740 [mini-batch 100 / 859]\n",
      "loss: 0.383337 [mini-batch 200 / 859]\n",
      "loss: 0.415562 [mini-batch 300 / 859]\n",
      "loss: 0.573697 [mini-batch 400 / 859]\n",
      "loss: 0.541405 [mini-batch 500 / 859]\n",
      "loss: 0.294764 [mini-batch 600 / 859]\n",
      "loss: 0.455765 [mini-batch 700 / 859]\n",
      "loss: 0.246504 [mini-batch 800 / 859]\n",
      "==> Accuracy: 84.6%, Avg loss: 0.440384\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 0.335727 [mini-batch 0 / 859]\n",
      "loss: 0.330975 [mini-batch 100 / 859]\n",
      "loss: 0.283058 [mini-batch 200 / 859]\n",
      "loss: 0.346410 [mini-batch 300 / 859]\n",
      "loss: 0.313314 [mini-batch 400 / 859]\n",
      "loss: 0.483508 [mini-batch 500 / 859]\n",
      "loss: 0.483733 [mini-batch 600 / 859]\n",
      "loss: 0.480652 [mini-batch 700 / 859]\n",
      "loss: 0.440148 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.5%, Avg loss: 0.415852\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 0.428354 [mini-batch 0 / 859]\n",
      "loss: 0.364188 [mini-batch 100 / 859]\n",
      "loss: 0.508800 [mini-batch 200 / 859]\n",
      "loss: 0.464897 [mini-batch 300 / 859]\n",
      "loss: 0.421649 [mini-batch 400 / 859]\n",
      "loss: 0.362799 [mini-batch 500 / 859]\n",
      "loss: 0.505817 [mini-batch 600 / 859]\n",
      "loss: 0.415369 [mini-batch 700 / 859]\n",
      "loss: 0.387236 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.8%, Avg loss: 0.408679\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "loss: 0.420482 [mini-batch 0 / 859]\n",
      "loss: 0.438140 [mini-batch 100 / 859]\n",
      "loss: 0.472859 [mini-batch 200 / 859]\n",
      "loss: 0.295129 [mini-batch 300 / 859]\n",
      "loss: 0.317660 [mini-batch 400 / 859]\n",
      "loss: 0.460630 [mini-batch 500 / 859]\n",
      "loss: 0.326293 [mini-batch 600 / 859]\n",
      "loss: 0.482378 [mini-batch 700 / 859]\n",
      "loss: 0.482112 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.9%, Avg loss: 0.414967\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "loss: 0.379826 [mini-batch 0 / 859]\n",
      "loss: 0.499722 [mini-batch 100 / 859]\n",
      "loss: 0.555131 [mini-batch 200 / 859]\n",
      "loss: 0.746206 [mini-batch 300 / 859]\n",
      "loss: 0.454915 [mini-batch 400 / 859]\n",
      "loss: 0.362258 [mini-batch 500 / 859]\n",
      "loss: 0.382578 [mini-batch 600 / 859]\n",
      "loss: 0.403127 [mini-batch 700 / 859]\n",
      "loss: 0.503345 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.2%, Avg loss: 0.422905\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "loss: 0.413737 [mini-batch 0 / 859]\n",
      "loss: 0.432626 [mini-batch 100 / 859]\n",
      "loss: 0.372278 [mini-batch 200 / 859]\n",
      "loss: 0.218535 [mini-batch 300 / 859]\n",
      "loss: 0.397769 [mini-batch 400 / 859]\n",
      "loss: 0.488748 [mini-batch 500 / 859]\n",
      "loss: 0.361037 [mini-batch 600 / 859]\n",
      "loss: 0.261967 [mini-batch 700 / 859]\n",
      "loss: 0.544543 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.4%, Avg loss: 0.426286\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "loss: 0.349804 [mini-batch 0 / 859]\n",
      "loss: 0.452616 [mini-batch 100 / 859]\n",
      "loss: 0.413466 [mini-batch 200 / 859]\n",
      "loss: 0.398184 [mini-batch 300 / 859]\n",
      "loss: 0.503572 [mini-batch 400 / 859]\n",
      "loss: 0.477186 [mini-batch 500 / 859]\n",
      "loss: 0.481736 [mini-batch 600 / 859]\n",
      "loss: 0.267134 [mini-batch 700 / 859]\n",
      "loss: 0.303377 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.6%, Avg loss: 0.419229\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "loss: 0.323113 [mini-batch 0 / 859]\n",
      "loss: 0.353699 [mini-batch 100 / 859]\n",
      "loss: 0.485282 [mini-batch 200 / 859]\n",
      "loss: 0.298983 [mini-batch 300 / 859]\n",
      "loss: 0.335060 [mini-batch 400 / 859]\n",
      "loss: 0.447367 [mini-batch 500 / 859]\n",
      "loss: 0.255489 [mini-batch 600 / 859]\n",
      "loss: 0.320932 [mini-batch 700 / 859]\n",
      "loss: 0.368703 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.3%, Avg loss: 0.421266\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "loss: 0.372926 [mini-batch 0 / 859]\n",
      "loss: 0.506177 [mini-batch 100 / 859]\n",
      "loss: 0.388932 [mini-batch 200 / 859]\n",
      "loss: 0.349976 [mini-batch 300 / 859]\n",
      "loss: 0.764176 [mini-batch 400 / 859]\n",
      "loss: 0.275543 [mini-batch 500 / 859]\n",
      "loss: 0.280516 [mini-batch 600 / 859]\n",
      "loss: 0.166830 [mini-batch 700 / 859]\n",
      "loss: 0.554373 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.5%, Avg loss: 0.420292\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "loss: 0.530404 [mini-batch 0 / 859]\n",
      "loss: 0.374400 [mini-batch 100 / 859]\n",
      "loss: 0.409961 [mini-batch 200 / 859]\n",
      "loss: 0.401117 [mini-batch 300 / 859]\n",
      "loss: 0.537064 [mini-batch 400 / 859]\n",
      "loss: 0.346078 [mini-batch 500 / 859]\n",
      "loss: 0.562276 [mini-batch 600 / 859]\n",
      "loss: 0.669916 [mini-batch 700 / 859]\n",
      "loss: 0.473773 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.6%, Avg loss: 0.424979\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "loss: 0.523283 [mini-batch 0 / 859]\n",
      "loss: 0.343491 [mini-batch 100 / 859]\n",
      "loss: 0.348172 [mini-batch 200 / 859]\n",
      "loss: 0.328572 [mini-batch 300 / 859]\n",
      "loss: 0.306886 [mini-batch 400 / 859]\n",
      "loss: 0.365511 [mini-batch 500 / 859]\n",
      "loss: 0.364829 [mini-batch 600 / 859]\n",
      "loss: 0.380061 [mini-batch 700 / 859]\n",
      "loss: 0.421354 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.4%, Avg loss: 0.424518\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "loss: 0.492291 [mini-batch 0 / 859]\n",
      "loss: 0.599468 [mini-batch 100 / 859]\n",
      "loss: 0.688808 [mini-batch 200 / 859]\n",
      "loss: 0.365549 [mini-batch 300 / 859]\n",
      "loss: 0.358572 [mini-batch 400 / 859]\n",
      "loss: 0.542631 [mini-batch 500 / 859]\n",
      "loss: 0.473533 [mini-batch 600 / 859]\n",
      "loss: 0.411345 [mini-batch 700 / 859]\n",
      "loss: 0.476515 [mini-batch 800 / 859]\n",
      "==> Accuracy: 85.0%, Avg loss: 0.428353\n",
      "\n",
      "Pre-train stats\n",
      "==> Accuracy: 11.9%, Avg loss: 5.281550\n",
      "\n",
      "Epoch 0\n",
      "--------------------\n",
      "loss: 5.216835 [mini-batch 0 / 859]\n",
      "loss: 0.866744 [mini-batch 100 / 859]\n",
      "loss: 0.668218 [mini-batch 200 / 859]\n",
      "loss: 0.397034 [mini-batch 300 / 859]\n",
      "loss: 0.592829 [mini-batch 400 / 859]\n",
      "loss: 0.598558 [mini-batch 500 / 859]\n",
      "loss: 0.537690 [mini-batch 600 / 859]\n",
      "loss: 0.644793 [mini-batch 700 / 859]\n",
      "loss: 0.578932 [mini-batch 800 / 859]\n",
      "==> Accuracy: 81.7%, Avg loss: 0.598250\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 0.527979 [mini-batch 0 / 859]\n",
      "loss: 0.620724 [mini-batch 100 / 859]\n",
      "loss: 0.633447 [mini-batch 200 / 859]\n",
      "loss: 0.560993 [mini-batch 300 / 859]\n",
      "loss: 0.621655 [mini-batch 400 / 859]\n",
      "loss: 0.731680 [mini-batch 500 / 859]\n",
      "loss: 0.672493 [mini-batch 600 / 859]\n",
      "loss: 0.781797 [mini-batch 700 / 859]\n",
      "loss: 0.709911 [mini-batch 800 / 859]\n",
      "==> Accuracy: 78.3%, Avg loss: 0.823625\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 0.960020 [mini-batch 0 / 859]\n",
      "loss: 0.770444 [mini-batch 100 / 859]\n",
      "loss: 0.763003 [mini-batch 200 / 859]\n",
      "loss: 0.758226 [mini-batch 300 / 859]\n",
      "loss: 0.906952 [mini-batch 400 / 859]\n",
      "loss: 0.861445 [mini-batch 500 / 859]\n",
      "loss: 0.693371 [mini-batch 600 / 859]\n",
      "loss: 0.605897 [mini-batch 700 / 859]\n",
      "loss: 0.970707 [mini-batch 800 / 859]\n",
      "==> Accuracy: 70.3%, Avg loss: 0.811924\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 0.749523 [mini-batch 0 / 859]\n",
      "loss: 0.919272 [mini-batch 100 / 859]\n",
      "loss: 0.720825 [mini-batch 200 / 859]\n",
      "loss: 0.752683 [mini-batch 300 / 859]\n",
      "loss: 0.817732 [mini-batch 400 / 859]\n",
      "loss: 1.013792 [mini-batch 500 / 859]\n",
      "loss: 0.714949 [mini-batch 600 / 859]\n",
      "loss: 0.666510 [mini-batch 700 / 859]\n",
      "loss: 0.704730 [mini-batch 800 / 859]\n",
      "==> Accuracy: 70.4%, Avg loss: 0.793554\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 1.035297 [mini-batch 0 / 859]\n",
      "loss: 0.862764 [mini-batch 100 / 859]\n",
      "loss: 0.978607 [mini-batch 200 / 859]\n",
      "loss: 0.655640 [mini-batch 300 / 859]\n",
      "loss: 0.703995 [mini-batch 400 / 859]\n",
      "loss: 0.795633 [mini-batch 500 / 859]\n",
      "loss: 0.675515 [mini-batch 600 / 859]\n",
      "loss: 0.864956 [mini-batch 700 / 859]\n",
      "loss: 0.740082 [mini-batch 800 / 859]\n",
      "==> Accuracy: 75.0%, Avg loss: 0.731192\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 0.807022 [mini-batch 0 / 859]\n",
      "loss: 0.681499 [mini-batch 100 / 859]\n",
      "loss: 0.790991 [mini-batch 200 / 859]\n",
      "loss: 0.838027 [mini-batch 300 / 859]\n",
      "loss: 0.714482 [mini-batch 400 / 859]\n",
      "loss: 0.797186 [mini-batch 500 / 859]\n",
      "loss: 0.605044 [mini-batch 600 / 859]\n",
      "loss: 0.662032 [mini-batch 700 / 859]\n",
      "loss: 0.798447 [mini-batch 800 / 859]\n",
      "==> Accuracy: 75.5%, Avg loss: 0.720766\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 0.578975 [mini-batch 0 / 859]\n",
      "loss: 0.756267 [mini-batch 100 / 859]\n",
      "loss: 0.697247 [mini-batch 200 / 859]\n",
      "loss: 0.657475 [mini-batch 300 / 859]\n",
      "loss: 0.673514 [mini-batch 400 / 859]\n",
      "loss: 0.762781 [mini-batch 500 / 859]\n",
      "loss: 0.598534 [mini-batch 600 / 859]\n",
      "loss: 0.677253 [mini-batch 700 / 859]\n",
      "loss: 0.741777 [mini-batch 800 / 859]\n",
      "==> Accuracy: 76.0%, Avg loss: 0.708319\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 0.617721 [mini-batch 0 / 859]\n",
      "loss: 1.042006 [mini-batch 100 / 859]\n",
      "loss: 0.621140 [mini-batch 200 / 859]\n",
      "loss: 0.823277 [mini-batch 300 / 859]\n",
      "loss: 0.491346 [mini-batch 400 / 859]\n",
      "loss: 0.811941 [mini-batch 500 / 859]\n",
      "loss: 0.740893 [mini-batch 600 / 859]\n",
      "loss: 0.634486 [mini-batch 700 / 859]\n",
      "loss: 0.703885 [mini-batch 800 / 859]\n",
      "==> Accuracy: 75.7%, Avg loss: 0.700170\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 0.772070 [mini-batch 0 / 859]\n",
      "loss: 0.665064 [mini-batch 100 / 859]\n",
      "loss: 0.531519 [mini-batch 200 / 859]\n",
      "loss: 0.534980 [mini-batch 300 / 859]\n",
      "loss: 0.689904 [mini-batch 400 / 859]\n",
      "loss: 0.656123 [mini-batch 500 / 859]\n",
      "loss: 0.656603 [mini-batch 600 / 859]\n",
      "loss: 0.535061 [mini-batch 700 / 859]\n",
      "loss: 0.744522 [mini-batch 800 / 859]\n",
      "==> Accuracy: 76.3%, Avg loss: 0.691960\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 0.868872 [mini-batch 0 / 859]\n",
      "loss: 0.591922 [mini-batch 100 / 859]\n",
      "loss: 0.731752 [mini-batch 200 / 859]\n",
      "loss: 0.838691 [mini-batch 300 / 859]\n",
      "loss: 0.778184 [mini-batch 400 / 859]\n",
      "loss: 0.678419 [mini-batch 500 / 859]\n",
      "loss: 0.656843 [mini-batch 600 / 859]\n",
      "loss: 0.773829 [mini-batch 700 / 859]\n",
      "loss: 0.640355 [mini-batch 800 / 859]\n",
      "==> Accuracy: 76.6%, Avg loss: 0.688120\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 0.925229 [mini-batch 0 / 859]\n",
      "loss: 0.709742 [mini-batch 100 / 859]\n",
      "loss: 0.770367 [mini-batch 200 / 859]\n",
      "loss: 0.632768 [mini-batch 300 / 859]\n",
      "loss: 0.735544 [mini-batch 400 / 859]\n",
      "loss: 0.733478 [mini-batch 500 / 859]\n",
      "loss: 0.850786 [mini-batch 600 / 859]\n",
      "loss: 0.484524 [mini-batch 700 / 859]\n",
      "loss: 0.763505 [mini-batch 800 / 859]\n",
      "==> Accuracy: 76.4%, Avg loss: 0.684701\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "loss: 0.946909 [mini-batch 0 / 859]\n",
      "loss: 0.719311 [mini-batch 100 / 859]\n",
      "loss: 0.661294 [mini-batch 200 / 859]\n",
      "loss: 0.717770 [mini-batch 300 / 859]\n",
      "loss: 0.701487 [mini-batch 400 / 859]\n",
      "loss: 0.795064 [mini-batch 500 / 859]\n",
      "loss: 0.869672 [mini-batch 600 / 859]\n",
      "loss: 0.963178 [mini-batch 700 / 859]\n",
      "loss: 0.759365 [mini-batch 800 / 859]\n",
      "==> Accuracy: 77.9%, Avg loss: 0.677996\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "loss: 0.695323 [mini-batch 0 / 859]\n",
      "loss: 0.726261 [mini-batch 100 / 859]\n",
      "loss: 0.647577 [mini-batch 200 / 859]\n",
      "loss: 0.766667 [mini-batch 300 / 859]\n",
      "loss: 0.555745 [mini-batch 400 / 859]\n",
      "loss: 0.796686 [mini-batch 500 / 859]\n",
      "loss: 0.622178 [mini-batch 600 / 859]\n",
      "loss: 0.575449 [mini-batch 700 / 859]\n",
      "loss: 0.810984 [mini-batch 800 / 859]\n",
      "==> Accuracy: 76.4%, Avg loss: 0.680237\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "loss: 0.817112 [mini-batch 0 / 859]\n",
      "loss: 0.672251 [mini-batch 100 / 859]\n",
      "loss: 0.587920 [mini-batch 200 / 859]\n",
      "loss: 0.574696 [mini-batch 300 / 859]\n",
      "loss: 0.780543 [mini-batch 400 / 859]\n",
      "loss: 0.671664 [mini-batch 500 / 859]\n",
      "loss: 0.652872 [mini-batch 600 / 859]\n",
      "loss: 0.921165 [mini-batch 700 / 859]\n",
      "loss: 0.709422 [mini-batch 800 / 859]\n",
      "==> Accuracy: 78.2%, Avg loss: 0.676965\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "loss: 0.796205 [mini-batch 0 / 859]\n",
      "loss: 0.919534 [mini-batch 100 / 859]\n",
      "loss: 0.696862 [mini-batch 200 / 859]\n",
      "loss: 0.576770 [mini-batch 300 / 859]\n",
      "loss: 0.978042 [mini-batch 400 / 859]\n",
      "loss: 0.536570 [mini-batch 500 / 859]\n",
      "loss: 0.759659 [mini-batch 600 / 859]\n",
      "loss: 1.030926 [mini-batch 700 / 859]\n",
      "loss: 0.633891 [mini-batch 800 / 859]\n",
      "==> Accuracy: 77.1%, Avg loss: 0.677082\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "loss: 0.637392 [mini-batch 0 / 859]\n",
      "loss: 0.865279 [mini-batch 100 / 859]\n",
      "loss: 0.830118 [mini-batch 200 / 859]\n",
      "loss: 0.776529 [mini-batch 300 / 859]\n",
      "loss: 0.732444 [mini-batch 400 / 859]\n",
      "loss: 0.769972 [mini-batch 500 / 859]\n",
      "loss: 0.496521 [mini-batch 600 / 859]\n",
      "loss: 0.643573 [mini-batch 700 / 859]\n",
      "loss: 0.769500 [mini-batch 800 / 859]\n",
      "==> Accuracy: 77.9%, Avg loss: 0.671755\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "loss: 0.614571 [mini-batch 0 / 859]\n",
      "loss: 0.987687 [mini-batch 100 / 859]\n",
      "loss: 0.723698 [mini-batch 200 / 859]\n",
      "loss: 0.789658 [mini-batch 300 / 859]\n",
      "loss: 0.577581 [mini-batch 400 / 859]\n",
      "loss: 0.668793 [mini-batch 500 / 859]\n",
      "loss: 0.576193 [mini-batch 600 / 859]\n",
      "loss: 0.715936 [mini-batch 700 / 859]\n",
      "loss: 0.649314 [mini-batch 800 / 859]\n",
      "==> Accuracy: 77.1%, Avg loss: 0.686551\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "loss: 0.817604 [mini-batch 0 / 859]\n",
      "loss: 0.787264 [mini-batch 100 / 859]\n",
      "loss: 0.822282 [mini-batch 200 / 859]\n",
      "loss: 0.812494 [mini-batch 300 / 859]\n",
      "loss: 0.714699 [mini-batch 400 / 859]\n",
      "loss: 0.716370 [mini-batch 500 / 859]\n",
      "loss: 0.602818 [mini-batch 600 / 859]\n",
      "loss: 0.582229 [mini-batch 700 / 859]\n",
      "loss: 0.674191 [mini-batch 800 / 859]\n",
      "==> Accuracy: 77.4%, Avg loss: 0.684862\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "loss: 0.809514 [mini-batch 0 / 859]\n",
      "loss: 0.831475 [mini-batch 100 / 859]\n",
      "loss: 0.561104 [mini-batch 200 / 859]\n",
      "loss: 0.783834 [mini-batch 300 / 859]\n",
      "loss: 0.631819 [mini-batch 400 / 859]\n",
      "loss: 0.738811 [mini-batch 500 / 859]\n",
      "loss: 0.686025 [mini-batch 600 / 859]\n",
      "loss: 0.748232 [mini-batch 700 / 859]\n",
      "loss: 0.854601 [mini-batch 800 / 859]\n",
      "==> Accuracy: 77.0%, Avg loss: 0.683129\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "loss: 0.699125 [mini-batch 0 / 859]\n",
      "loss: 0.732446 [mini-batch 100 / 859]\n",
      "loss: 0.748723 [mini-batch 200 / 859]\n",
      "loss: 0.737342 [mini-batch 300 / 859]\n",
      "loss: 0.483034 [mini-batch 400 / 859]\n",
      "loss: 0.702404 [mini-batch 500 / 859]\n",
      "loss: 0.780922 [mini-batch 600 / 859]\n",
      "loss: 0.588402 [mini-batch 700 / 859]\n",
      "loss: 0.744789 [mini-batch 800 / 859]\n",
      "==> Accuracy: 77.1%, Avg loss: 0.679940\n",
      "\n",
      "Pre-train stats\n",
      "==> Accuracy: 11.1%, Avg loss: 6.001073\n",
      "\n",
      "Epoch 0\n",
      "--------------------\n",
      "loss: 6.208683 [mini-batch 0 / 859]\n",
      "loss: 0.751642 [mini-batch 100 / 859]\n",
      "loss: 0.914499 [mini-batch 200 / 859]\n",
      "loss: 1.435101 [mini-batch 300 / 859]\n",
      "loss: 2.013181 [mini-batch 400 / 859]\n",
      "loss: 2.096410 [mini-batch 500 / 859]\n",
      "loss: 2.006250 [mini-batch 600 / 859]\n",
      "loss: 1.899933 [mini-batch 700 / 859]\n",
      "loss: 1.790961 [mini-batch 800 / 859]\n",
      "==> Accuracy: 20.3%, Avg loss: 1.748890\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 1.662825 [mini-batch 0 / 859]\n",
      "loss: 1.785359 [mini-batch 100 / 859]\n",
      "loss: 1.742741 [mini-batch 200 / 859]\n",
      "loss: 1.649146 [mini-batch 300 / 859]\n",
      "loss: 1.713862 [mini-batch 400 / 859]\n",
      "loss: 1.773040 [mini-batch 500 / 859]\n",
      "loss: 1.639016 [mini-batch 600 / 859]\n",
      "loss: 1.638823 [mini-batch 700 / 859]\n",
      "loss: 1.706954 [mini-batch 800 / 859]\n",
      "==> Accuracy: 21.0%, Avg loss: 1.695009\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 1.808754 [mini-batch 0 / 859]\n",
      "loss: 1.738307 [mini-batch 100 / 859]\n",
      "loss: 1.701618 [mini-batch 200 / 859]\n",
      "loss: 1.695294 [mini-batch 300 / 859]\n",
      "loss: 1.672873 [mini-batch 400 / 859]\n",
      "loss: 1.707087 [mini-batch 500 / 859]\n",
      "loss: 1.662780 [mini-batch 600 / 859]\n",
      "loss: 1.741541 [mini-batch 700 / 859]\n",
      "loss: 1.706230 [mini-batch 800 / 859]\n",
      "==> Accuracy: 21.0%, Avg loss: 1.682809\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 1.756549 [mini-batch 0 / 859]\n",
      "loss: 1.644760 [mini-batch 100 / 859]\n",
      "loss: 1.589995 [mini-batch 200 / 859]\n",
      "loss: 1.604984 [mini-batch 300 / 859]\n",
      "loss: 1.732651 [mini-batch 400 / 859]\n",
      "loss: 1.595958 [mini-batch 500 / 859]\n",
      "loss: 1.608899 [mini-batch 600 / 859]\n",
      "loss: 1.668916 [mini-batch 700 / 859]\n",
      "loss: 1.728742 [mini-batch 800 / 859]\n",
      "==> Accuracy: 20.2%, Avg loss: 1.672346\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 1.705441 [mini-batch 0 / 859]\n",
      "loss: 1.733883 [mini-batch 100 / 859]\n",
      "loss: 1.667018 [mini-batch 200 / 859]\n",
      "loss: 1.704729 [mini-batch 300 / 859]\n",
      "loss: 1.670299 [mini-batch 400 / 859]\n",
      "loss: 1.728942 [mini-batch 500 / 859]\n",
      "loss: 1.623657 [mini-batch 600 / 859]\n",
      "loss: 1.715094 [mini-batch 700 / 859]\n",
      "loss: 1.626477 [mini-batch 800 / 859]\n",
      "==> Accuracy: 19.9%, Avg loss: 1.687896\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 1.671162 [mini-batch 0 / 859]\n",
      "loss: 1.616682 [mini-batch 100 / 859]\n",
      "loss: 1.678575 [mini-batch 200 / 859]\n",
      "loss: 1.603925 [mini-batch 300 / 859]\n",
      "loss: 1.703764 [mini-batch 400 / 859]\n",
      "loss: 1.647177 [mini-batch 500 / 859]\n",
      "loss: 1.747080 [mini-batch 600 / 859]\n",
      "loss: 1.716692 [mini-batch 700 / 859]\n",
      "loss: 1.657742 [mini-batch 800 / 859]\n",
      "==> Accuracy: 23.5%, Avg loss: 1.674101\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 1.644034 [mini-batch 0 / 859]\n",
      "loss: 1.744191 [mini-batch 100 / 859]\n",
      "loss: 1.678760 [mini-batch 200 / 859]\n",
      "loss: 1.732715 [mini-batch 300 / 859]\n",
      "loss: 1.670852 [mini-batch 400 / 859]\n",
      "loss: 1.707162 [mini-batch 500 / 859]\n",
      "loss: 1.612612 [mini-batch 600 / 859]\n",
      "loss: 1.702371 [mini-batch 700 / 859]\n",
      "loss: 1.673804 [mini-batch 800 / 859]\n",
      "==> Accuracy: 25.4%, Avg loss: 1.668022\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 1.712387 [mini-batch 0 / 859]\n",
      "loss: 1.734429 [mini-batch 100 / 859]\n",
      "loss: 1.642612 [mini-batch 200 / 859]\n",
      "loss: 1.722078 [mini-batch 300 / 859]\n",
      "loss: 1.646933 [mini-batch 400 / 859]\n",
      "loss: 1.671013 [mini-batch 500 / 859]\n",
      "loss: 1.638236 [mini-batch 600 / 859]\n",
      "loss: 1.770344 [mini-batch 700 / 859]\n",
      "loss: 1.731997 [mini-batch 800 / 859]\n",
      "==> Accuracy: 23.2%, Avg loss: 1.654666\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 1.682740 [mini-batch 0 / 859]\n",
      "loss: 1.692884 [mini-batch 100 / 859]\n",
      "loss: 1.700547 [mini-batch 200 / 859]\n",
      "loss: 1.783217 [mini-batch 300 / 859]\n",
      "loss: 1.556431 [mini-batch 400 / 859]\n",
      "loss: 1.726609 [mini-batch 500 / 859]\n",
      "loss: 1.582317 [mini-batch 600 / 859]\n",
      "loss: 1.763408 [mini-batch 700 / 859]\n",
      "loss: 1.660496 [mini-batch 800 / 859]\n",
      "==> Accuracy: 24.1%, Avg loss: 1.676645\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 1.668748 [mini-batch 0 / 859]\n",
      "loss: 1.619596 [mini-batch 100 / 859]\n",
      "loss: 1.615254 [mini-batch 200 / 859]\n",
      "loss: 1.711843 [mini-batch 300 / 859]\n",
      "loss: 1.701481 [mini-batch 400 / 859]\n",
      "loss: 1.617071 [mini-batch 500 / 859]\n",
      "loss: 1.572981 [mini-batch 600 / 859]\n",
      "loss: 1.778426 [mini-batch 700 / 859]\n",
      "loss: 1.560508 [mini-batch 800 / 859]\n",
      "==> Accuracy: 23.5%, Avg loss: 1.664426\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 1.700665 [mini-batch 0 / 859]\n",
      "loss: 1.503939 [mini-batch 100 / 859]\n",
      "loss: 1.606915 [mini-batch 200 / 859]\n",
      "loss: 1.667903 [mini-batch 300 / 859]\n",
      "loss: 1.672187 [mini-batch 400 / 859]\n",
      "loss: 1.656365 [mini-batch 500 / 859]\n",
      "loss: 1.729788 [mini-batch 600 / 859]\n",
      "loss: 1.630295 [mini-batch 700 / 859]\n",
      "loss: 1.565637 [mini-batch 800 / 859]\n",
      "==> Accuracy: 24.0%, Avg loss: 1.651624\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "loss: 1.717185 [mini-batch 0 / 859]\n",
      "loss: 1.708976 [mini-batch 100 / 859]\n",
      "loss: 1.672590 [mini-batch 200 / 859]\n",
      "loss: 1.676197 [mini-batch 300 / 859]\n",
      "loss: 1.601474 [mini-batch 400 / 859]\n",
      "loss: 1.650779 [mini-batch 500 / 859]\n",
      "loss: 1.706790 [mini-batch 600 / 859]\n",
      "loss: 1.637769 [mini-batch 700 / 859]\n",
      "loss: 1.645315 [mini-batch 800 / 859]\n",
      "==> Accuracy: 25.7%, Avg loss: 1.648913\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "loss: 1.670746 [mini-batch 0 / 859]\n",
      "loss: 1.656565 [mini-batch 100 / 859]\n",
      "loss: 1.676234 [mini-batch 200 / 859]\n",
      "loss: 1.660796 [mini-batch 300 / 859]\n",
      "loss: 1.713650 [mini-batch 400 / 859]\n",
      "loss: 1.595439 [mini-batch 500 / 859]\n",
      "loss: 1.708800 [mini-batch 600 / 859]\n",
      "loss: 1.713148 [mini-batch 700 / 859]\n",
      "loss: 1.664793 [mini-batch 800 / 859]\n",
      "==> Accuracy: 23.8%, Avg loss: 1.643744\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "loss: 1.562151 [mini-batch 0 / 859]\n",
      "loss: 1.605627 [mini-batch 100 / 859]\n",
      "loss: 1.724806 [mini-batch 200 / 859]\n",
      "loss: 1.671443 [mini-batch 300 / 859]\n",
      "loss: 1.691156 [mini-batch 400 / 859]\n",
      "loss: 1.580697 [mini-batch 500 / 859]\n",
      "loss: 1.675236 [mini-batch 600 / 859]\n",
      "loss: 1.657578 [mini-batch 700 / 859]\n",
      "loss: 1.684938 [mini-batch 800 / 859]\n",
      "==> Accuracy: 29.2%, Avg loss: 1.653927\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "loss: 1.742149 [mini-batch 0 / 859]\n",
      "loss: 1.628909 [mini-batch 100 / 859]\n",
      "loss: 1.637400 [mini-batch 200 / 859]\n",
      "loss: 1.611487 [mini-batch 300 / 859]\n",
      "loss: 1.666166 [mini-batch 400 / 859]\n",
      "loss: 1.685976 [mini-batch 500 / 859]\n",
      "loss: 1.572926 [mini-batch 600 / 859]\n",
      "loss: 1.654258 [mini-batch 700 / 859]\n",
      "loss: 1.729345 [mini-batch 800 / 859]\n",
      "==> Accuracy: 23.4%, Avg loss: 1.639971\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "loss: 1.732688 [mini-batch 0 / 859]\n",
      "loss: 1.654447 [mini-batch 100 / 859]\n",
      "loss: 1.601835 [mini-batch 200 / 859]\n",
      "loss: 1.666496 [mini-batch 300 / 859]\n",
      "loss: 1.624136 [mini-batch 400 / 859]\n",
      "loss: 1.635537 [mini-batch 500 / 859]\n",
      "loss: 1.704614 [mini-batch 600 / 859]\n",
      "loss: 1.598856 [mini-batch 700 / 859]\n",
      "loss: 1.765956 [mini-batch 800 / 859]\n",
      "==> Accuracy: 29.0%, Avg loss: 1.640658\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "loss: 1.646805 [mini-batch 0 / 859]\n",
      "loss: 1.689887 [mini-batch 100 / 859]\n",
      "loss: 1.714110 [mini-batch 200 / 859]\n",
      "loss: 1.761411 [mini-batch 300 / 859]\n",
      "loss: 1.646384 [mini-batch 400 / 859]\n",
      "loss: 1.641888 [mini-batch 500 / 859]\n",
      "loss: 1.834455 [mini-batch 600 / 859]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guille/Documents/fashion_mnist_nn/regularization/sgd_L1.py:70: RuntimeWarning: divide by zero encountered in log\n",
      "  return - (y * np.log(self.a[-1])).sum()\n",
      "/home/guille/Documents/fashion_mnist_nn/regularization/sgd_L1.py:70: RuntimeWarning: invalid value encountered in multiply\n",
      "  return - (y * np.log(self.a[-1])).sum()\n",
      "/home/guille/Documents/fashion_mnist_nn/regularization/sgd_L1.py:65: RuntimeWarning: overflow encountered in matmul\n",
      "  self.z[l] = self.weights[l] @ self.a[l-1] + self.biases[l]\n",
      "/home/guille/venv/torch/lib/python3.11/site-packages/scipy/special/_logsumexp.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  exp_x_shifted = np.exp(x - x_max)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:      nan [mini-batch 700 / 859]\n",
      "loss:      nan [mini-batch 800 / 859]\n",
      "==> Accuracy: 9.5%, Avg loss:      nan\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 859]\n",
      "loss:      nan [mini-batch 100 / 859]\n",
      "loss:      nan [mini-batch 200 / 859]\n",
      "loss:      nan [mini-batch 300 / 859]\n",
      "loss:      nan [mini-batch 400 / 859]\n",
      "loss:      nan [mini-batch 500 / 859]\n",
      "loss:      nan [mini-batch 600 / 859]\n",
      "loss:      nan [mini-batch 700 / 859]\n",
      "loss:      nan [mini-batch 800 / 859]\n",
      "==> Accuracy: 9.5%, Avg loss:      nan\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 859]\n",
      "loss:      nan [mini-batch 100 / 859]\n",
      "loss:      nan [mini-batch 200 / 859]\n",
      "loss:      nan [mini-batch 300 / 859]\n",
      "loss:      nan [mini-batch 400 / 859]\n",
      "loss:      nan [mini-batch 500 / 859]\n",
      "loss:      nan [mini-batch 600 / 859]\n",
      "loss:      nan [mini-batch 700 / 859]\n",
      "loss:      nan [mini-batch 800 / 859]\n",
      "==> Accuracy: 9.5%, Avg loss:      nan\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 859]\n",
      "loss:      nan [mini-batch 100 / 859]\n",
      "loss:      nan [mini-batch 200 / 859]\n",
      "loss:      nan [mini-batch 300 / 859]\n",
      "loss:      nan [mini-batch 400 / 859]\n",
      "loss:      nan [mini-batch 500 / 859]\n",
      "loss:      nan [mini-batch 600 / 859]\n",
      "loss:      nan [mini-batch 700 / 859]\n",
      "loss:      nan [mini-batch 800 / 859]\n",
      "==> Accuracy: 9.5%, Avg loss:      nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.001, 0.01, 0.1, 1, 5]\n",
    "\n",
    "result = []\n",
    "for l in lambdas:\n",
    "    nn = NeuralNetwork(\n",
    "        [28*28, 1024, 512, 128, 10], # layers size\n",
    "        1e-3,                        # learning rate\n",
    "        l,                           # L1 lambda\n",
    "        64,                          # mini batch size\n",
    "        20                           # training epochs\n",
    "    )\n",
    "    accuracy, _ = nn.train(train_data, validation_data)\n",
    "    result.append( accuracy[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991a4bf3-65e7-445d-bc36-1906a9b85db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lambdas, result, \"+\")\n",
    "plt.xscale(\"log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
