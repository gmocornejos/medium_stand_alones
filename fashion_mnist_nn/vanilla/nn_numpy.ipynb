{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa85d055-6560-4181-b559-7810c2962476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d52d7e6-3722-4cfd-aa35-570eb5efccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train_data.pkl\", \"rb\") as train_file:\n",
    "    train_data = pickle.load(train_file)\n",
    "\n",
    "with open(\"data/test_data.pkl\", \"rb\") as test_file:\n",
    "    test_data = pickle.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef8a735-6896-4a23-9fe1-115f0eb9a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "\n",
    "img_px = 28 # FashinMNIST images are 28x28 pixels\n",
    "n_i = 512   # Input layer, from 28*28 to 512 units\n",
    "n_1 = 128   # Layer 1, from 512 to 128 units\n",
    "n_2 = 64    # Layer 2, from 128 to 64 units\n",
    "n_L = 10    # Layer L, from 64 to 10 units, to match the 10 classes in FashionMNIST\n",
    "\n",
    "# To initialize the weights and biases, \n",
    "# it's usual to draw from a uniform distribution\n",
    "# with boundaries defined by the layer-input size,\n",
    "# as below\n",
    "k_i = np.sqrt( 1 /(img_px * img_px) )\n",
    "k_1 = np.sqrt( 1 / n_i )\n",
    "k_2 = np.sqrt( 1 / n_1)\n",
    "k_L = np.sqrt( 1 / n_2 )\n",
    "\n",
    "w_i = rng.uniform( -k_i, k_i, (n_i, img_px*img_px) )\n",
    "b_i = rng.normal( -k_i, k_i, size=n_i )\n",
    "\n",
    "w_1 = rng.normal( -k_1, k_1, size=(n_1, n_i) )\n",
    "b_1 = rng.normal( -k_1, k_1, size=n_1 )\n",
    "\n",
    "w_2 = rng.normal( -k_2, k_2, size=(n_2, n_1))\n",
    "b_2 = rng.normal( -k_2, k_2, size=n_2 )\n",
    "\n",
    "w_L = rng.normal( -k_L, k_L, size=(n_L, n_2) )\n",
    "b_L = rng.normal( -k_L, k_L, size=n_L )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4cfbfd-75e2-4cba-b0f0-e78dbbb96224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09958217, 0.0978552 , 0.10078088, 0.09859859, 0.09980073,\n",
       "       0.0980186 , 0.10177862, 0.10044439, 0.10187438, 0.10126645])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward function, used to make predictions\n",
    "\n",
    "def forward(x, return_z_a=False):\n",
    "    x = x.flatten()\n",
    "    \n",
    "    z_i = w_i @ x + b_i\n",
    "    a_i = np.maximum(0, z_i)\n",
    "\n",
    "    z_1 = w_1 @ a_i + b_1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    z_2 = w_2 @ a_1 + b_2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    z_L = w_L @ a_2 + b_L\n",
    "    a_L = softmax(z_L)\n",
    "\n",
    "    if return_z_a:\n",
    "        return (z_i, z_1, z_2, z_L), (a_i, a_1, a_2, a_L)\n",
    "    else:\n",
    "        return a_L\n",
    "\n",
    "forward(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c13a1b2-0959-4e7d-9bdf-a7fe5f3a0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, 2.302684260091983)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-entropy loss\n",
    "\n",
    "def loss_fn(y, a_L):\n",
    "    return - (y * np.log(a_L)).sum()   \n",
    "\n",
    "\n",
    "def test():\n",
    "    n_samples = len(test_data)\n",
    "    correct = 0\n",
    "    sum_loss = 0\n",
    "    for x, y in test_data:\n",
    "        y_hat = forward(x)\n",
    "        correct += y.argmax() == y_hat.argmax()\n",
    "        sum_loss += loss_fn(y, y_hat)\n",
    "    return correct / n_samples, sum_loss / n_samples        \n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3124b8-22cd-438d-94dd-58c9e2caf634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_derivative(x):\n",
    "    r = np.ones(x.shape)\n",
    "    r[x == 0] = 0\n",
    "    return r\n",
    "\n",
    "def calc_derivatives(x, y):\n",
    "    x = x.flatten()\n",
    "    \n",
    "    (z_i, z_1, z_2, z_L), (a_i, a_1, a_2, a_L) = forward(x, True)\n",
    "    \n",
    "    nabla_z_L = a_L - y\n",
    "    nabla_w_L = np.outer(nabla_z_L, a_2)\n",
    "    \n",
    "    nabla_z_2 = ReLU_derivative(z_2) * (w_L.T @ nabla_z_L)\n",
    "    nabla_w_2 = np.outer(nabla_z_2, a_1)\n",
    "    \n",
    "    nabla_z_1 = ReLU_derivative(z_1) * (w_2.T @ nabla_z_2)\n",
    "    nabla_w_1 = np.outer(nabla_z_1, a_i)\n",
    "    \n",
    "    nabla_z_i = ReLU_derivative(z_i) * (w_1.T @ nabla_z_1)\n",
    "    nabla_w_i = np.outer(nabla_z_i, x)\n",
    "\n",
    "    return (nabla_w_i, nabla_w_1, nabla_w_2, nabla_w_L), (nabla_z_i, nabla_z_1, nabla_z_2, nabla_z_L), loss_fn(y, a_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a18c46-5ec1-474c-ba45-35f7a46cf21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train stats\n",
      "==> Accuracy: 10.0%, Avg loss: 2.302684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamters\n",
    "\n",
    "learning_rate = 1e-3\n",
    "mini_batch_size = 64\n",
    "training_epochs = 5\n",
    "\n",
    "n_mini_batches = len(train_data) // mini_batch_size\n",
    "\n",
    "\n",
    "# Performance baseline\n",
    "\n",
    "print(\"Pre-train stats\")\n",
    "accuracy, avg_loss = test()\n",
    "print(f\"==> Accuracy: {100*accuracy:0.1f}%, Avg loss: {avg_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35857ef2-0802-4fc1-9823-13655ff8ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "--------------------\n",
      "loss: 2.304699 [mini-batch 0 / 937]\n",
      "loss: 2.302388 [mini-batch 100 / 937]\n",
      "loss: 2.301739 [mini-batch 200 / 937]\n",
      "loss: 2.218086 [mini-batch 300 / 937]\n",
      "loss: 2.143030 [mini-batch 400 / 937]\n",
      "loss: 1.750400 [mini-batch 500 / 937]\n",
      "loss: 1.242420 [mini-batch 600 / 937]\n",
      "loss: 1.079896 [mini-batch 700 / 937]\n",
      "loss: 0.949014 [mini-batch 800 / 937]\n",
      "loss: 0.928119 [mini-batch 900 / 937]\n",
      "==> Accuracy: 65.2%, Avg loss: 0.908876\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 0.876424 [mini-batch 0 / 937]\n",
      "loss: 0.933463 [mini-batch 100 / 937]\n",
      "loss: 0.676525 [mini-batch 200 / 937]\n",
      "loss: 0.914220 [mini-batch 300 / 937]\n",
      "loss: 0.775485 [mini-batch 400 / 937]\n",
      "loss: 0.653926 [mini-batch 500 / 937]\n",
      "loss: 0.749385 [mini-batch 600 / 937]\n",
      "loss: 0.764485 [mini-batch 700 / 937]\n",
      "loss: 0.728813 [mini-batch 800 / 937]\n",
      "loss: 0.710892 [mini-batch 900 / 937]\n",
      "==> Accuracy: 73.8%, Avg loss: 0.696800\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 0.685634 [mini-batch 0 / 937]\n",
      "loss: 0.726217 [mini-batch 100 / 937]\n",
      "loss: 0.500948 [mini-batch 200 / 937]\n",
      "loss: 0.745089 [mini-batch 300 / 937]\n",
      "loss: 0.749378 [mini-batch 400 / 937]\n",
      "loss: 0.529310 [mini-batch 500 / 937]\n",
      "loss: 0.631517 [mini-batch 600 / 937]\n",
      "loss: 0.707054 [mini-batch 700 / 937]\n",
      "loss: 0.688108 [mini-batch 800 / 937]\n",
      "loss: 0.581789 [mini-batch 900 / 937]\n",
      "==> Accuracy: 78.5%, Avg loss: 0.606268\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 0.559227 [mini-batch 0 / 937]\n",
      "loss: 0.632850 [mini-batch 100 / 937]\n",
      "loss: 0.433026 [mini-batch 200 / 937]\n",
      "loss: 0.651799 [mini-batch 300 / 937]\n",
      "loss: 0.769764 [mini-batch 400 / 937]\n",
      "loss: 0.485998 [mini-batch 500 / 937]\n",
      "loss: 0.597465 [mini-batch 600 / 937]\n",
      "loss: 0.674659 [mini-batch 700 / 937]\n",
      "loss: 0.652619 [mini-batch 800 / 937]\n",
      "loss: 0.522267 [mini-batch 900 / 937]\n",
      "==> Accuracy: 80.5%, Avg loss: 0.566198\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 0.442185 [mini-batch 0 / 937]\n",
      "loss: 0.566177 [mini-batch 100 / 937]\n",
      "loss: 0.410584 [mini-batch 200 / 937]\n",
      "loss: 0.607202 [mini-batch 300 / 937]\n",
      "loss: 0.737018 [mini-batch 400 / 937]\n",
      "loss: 0.469365 [mini-batch 500 / 937]\n",
      "loss: 0.545741 [mini-batch 600 / 937]\n",
      "loss: 0.640569 [mini-batch 700 / 937]\n",
      "loss: 0.618707 [mini-batch 800 / 937]\n",
      "loss: 0.479805 [mini-batch 900 / 937]\n",
      "==> Accuracy: 80.9%, Avg loss: 0.545726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    print(f\"Epoch {epoch}\\n\" + 20*'-')\n",
    "    rng.shuffle(training_index)\n",
    "    for mini_batch_number in range(n_mini_batches):\n",
    "        # batch average derivatives\n",
    "        sum_nabla_w_i = np.zeros(w_i.shape)\n",
    "        sum_nabla_w_1 = np.zeros(w_1.shape)\n",
    "        sum_nabla_w_2 = np.zeros(w_2.shape)\n",
    "        sum_nabla_w_L = np.zeros(w_L.shape)\n",
    "        sum_nabla_b_i = np.zeros(b_i.shape)\n",
    "        sum_nabla_b_1 = np.zeros(b_1.shape)\n",
    "        sum_nabla_b_2 = np.zeros(b_2.shape)\n",
    "        sum_nabla_b_L = np.zeros(b_L.shape)\n",
    "        sum_loss = 0\n",
    "        mini_batch = train_data[mini_batch_number*mini_batch_size : (mini_batch_number+1)*mini_batch_size]\n",
    "        for x, y in mini_batch:\n",
    "            (nabla_w_i, nabla_w_1, nabla_w_2, nabla_w_L), (nabla_b_i, nabla_b_1, nabla_b_2, nabla_b_L), loss = calc_derivatives(x, y)\n",
    "            sum_nabla_w_i += nabla_w_i\n",
    "            sum_nabla_w_1 += nabla_w_1\n",
    "            sum_nabla_w_2 += nabla_w_2\n",
    "            sum_nabla_w_L += nabla_w_L\n",
    "            sum_nabla_b_i += nabla_b_i\n",
    "            sum_nabla_b_1 += nabla_b_1\n",
    "            sum_nabla_b_2 += nabla_b_2\n",
    "            sum_nabla_b_L += nabla_b_L\n",
    "            sum_loss += loss\n",
    "        w_i -= learning_rate * (sum_nabla_w_i / len(mini_batch))\n",
    "        w_1 -= learning_rate * (sum_nabla_w_1 / len(mini_batch))\n",
    "        w_2 -= learning_rate * (sum_nabla_w_2 / len(mini_batch))\n",
    "        w_L -= learning_rate * (sum_nabla_w_L / len(mini_batch))\n",
    "        b_i -= learning_rate * (sum_nabla_b_i / len(mini_batch))\n",
    "        b_1 -= learning_rate * (sum_nabla_b_1 / len(mini_batch))\n",
    "        b_2 -= learning_rate * (sum_nabla_b_2 / len(mini_batch))\n",
    "        b_L -= learning_rate * (sum_nabla_b_L / len(mini_batch))\n",
    "        if mini_batch_number % 100 == 0:\n",
    "            print(f\"loss: {sum_loss/len(mini_batch):>8f} [mini-batch {mini_batch_number} / {n_mini_batches}]\")\n",
    "    # Test at the end of each epoch\n",
    "    accuracy, avg_loss = test()\n",
    "    print(f\"==> Accuracy: {100*accuracy:0.1f}%, Avg loss: {avg_loss:>8f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
